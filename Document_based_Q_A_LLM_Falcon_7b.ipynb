{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 285,
      "metadata": {
        "id": "NrgfLZpMIgyw"
      },
      "outputs": [],
      "source": [
        "# ! pip install -U pypdf torch transformers langchain ipywidgets accelerate \\\n",
        "#  sentence_transformers pyarrow pandas bitsandbytes einops xformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInferenceAPIEmbeddings\n",
        "from langchain.vectorstores import SKLearnVectorStore"
      ],
      "metadata": {
        "id": "EyafiqsoI7Mm"
      },
      "execution_count": 286,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from google.colab import output\n",
        "#output.enable_custom_widget_manager()"
      ],
      "metadata": {
        "id": "r9Pimkd25UZW"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the PDF file. English or French only\n",
        "pdf_doc_path=''\n",
        "if pdf_doc_path:\n",
        "  loader = PyPDFLoader(pdf_doc_path)\n",
        "  pages = loader.load_and_split()\n",
        "\n",
        "  text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=64,\n",
        "    separators=['\\n\\n', '\\n', '(?=>\\. )', ' ', ''])\n",
        "\n",
        "  # Split the pages into texts as defined above\n",
        "  texts = text_splitter.split_documents(pages)\n",
        "\n",
        "  embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n"
      ],
      "metadata": {
        "id": "rURrisgIJTil"
      },
      "execution_count": 288,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the persisted vector store\n",
        "vector_db_path = \"./document_vector_db.parquet\"\n",
        "if pdf_doc_path:\n",
        "# Create/upadte the vector store\n",
        "  vector_db = SKLearnVectorStore.from_documents(\n",
        "      texts,\n",
        "      embedding=embeddings,\n",
        "      persist_path=vector_db_path,\n",
        "      serializer=\"parquet\")\n",
        "  # persist the store\n",
        "  vector_db.persist()\n",
        "else:\n",
        "    vector_db=SKLearnVectorStore(\n",
        "        embedding=embeddings,\n",
        "        persist_path=vector_db_path,\n",
        "        serializer=\"parquet\"\n",
        "    )"
      ],
      "metadata": {
        "id": "0NqFya_3kVdc"
      },
      "execution_count": 289,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# load into pandas\n",
        "df = pd.read_parquet(vector_db_path)\n",
        "\n",
        "# Have a look at the store and remove dublicates\n",
        "df=df.drop_duplicates(subset=\"texts\")"
      ],
      "metadata": {
        "id": "zu0-zpEDQZiv"
      },
      "execution_count": 290,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain import HuggingFaceHub\n",
        "import os\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"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\"\n",
        "\n",
        "llm=HuggingFaceHub(repo_id=\"tiiuae/falcon-7b\",\n",
        "                   model_kwargs={\"temperature\":0.5 ,\n",
        "                                 \"max_length\":512,\n",
        "                                 \"max_new_tokens\":200\n",
        "                                 })"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BaWKCSo84DNI",
        "outputId": "1907b127-6ddd-4c95-a687-197a6a4843e8"
      },
      "execution_count": 291,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
            "  warnings.warn(warning_message, FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",\n",
        "                                 retriever=vector_db.as_retriever(search_kwargs={\"k\": 1}),\n",
        "                                 return_source_documents=True,\n",
        "                                 verbose=False,\n",
        ")\n"
      ],
      "metadata": {
        "id": "ASPbYcmLFCdC"
      },
      "execution_count": 292,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Ask a question?\n",
        "queary=''\n",
        "results=qa({\"query\": queary})"
      ],
      "metadata": {
        "id": "IZ8SS7DWFYLQ"
      },
      "execution_count": 297,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "i8dQHKVBLm9y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}